# -*- coding: utf-8 -*-
"""project misis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ut1NrXswlhVyZZSeAeXsMpRk_UdalEwi
"""

import kagglehub
import os
import pandas as pd

path = kagglehub.dataset_download("datasnaek/youtube-new")
print("Path to dataset files:", path)

csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]
print("Найдены файлы:", csv_files)

dfs = []
for file in csv_files:
    full_path = os.path.join(path, file)
    try:
        df_temp = pd.read_csv(full_path, encoding='utf-8', on_bad_lines='skip', low_memory=False)
    except UnicodeDecodeError:
        try:
            df_temp = pd.read_csv(full_path, encoding='latin1', on_bad_lines='skip', low_memory=False)
        except Exception as e:
            print(f"Не удалось прочитать файл {file}: {e}")
            continue
    dfs.append(df_temp)
    print(f"Загружен файл: {file}, строк: {len(df_temp)}")

df = pd.concat(dfs, ignore_index=True)
print("\nОбщий размер датасета:", df.shape)

usecols = ['category_id', 'views', 'likes', 'dislikes', 'comment_count']

for file in csv_files:
    full_path = os.path.join(path, file)
    try:
        df_temp = pd.read_csv(
            full_path,
            usecols=usecols,
            encoding='utf-8',
            on_bad_lines='skip',
            low_memory=False
        )
    except (UnicodeDecodeError, ValueError):
        try:
            df_temp = pd.read_csv(
                full_path,
                usecols=usecols,
                encoding='latin1',
                on_bad_lines='skip',
                low_memory=False
            )
        except Exception as e:
            print(f"Пропущен файл {file}: {e}")
            continue
    dfs.append(df_temp)

print(df[['views', 'likes', 'dislikes', 'comment_count']].head())
print(df['category_id'].value_counts().head())

df['views'] = df['views'].astype(str).str.replace(',', '').astype(float)

print("Столбцы датасета:")
print(df.columns.tolist())
print("\nПервые 3 строки:")
print(df.head(3))

print("\nИнформация о датафрейме:")
print(df.info())

if 'country' not in df.columns:
    pass

numeric_cols = ['views', 'likes', 'dislikes', 'comment_count']

for col in numeric_cols:
    if df[col].dtype == 'object':
        df[col] = df[col].astype(str).str.replace(',', '', regex=False)
    df[col] = pd.to_numeric(df[col], errors='coerce')

df = df[
    (df['views'] > 0) &
    (df['likes'] >= 0) &
    (df['dislikes'] >= 0) &
    (df['comment_count'] >= 0)
].copy()

df['category_id'] = pd.to_numeric(df['category_id'], errors='coerce')
df = df.dropna(subset=['views', 'likes', 'category_id'])

df = df.reset_index(drop=True)

print(f"\n После очистки осталось: {len(df)} видео")
print(f"\nДиапазон просмотров: от {df['views'].min():,} до {df['views'].max():,}")
print(f"Среднее количество просмотров: {df['views'].mean():,.0f}")

import matplotlib.pyplot as plt
import seaborn as sns

plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(df['views'], df['likes'], alpha=0.4, s=10)
plt.xscale('log')
plt.yscale('log')
plt.xlabel('Просмотры (лог)')
plt.ylabel('Лайки (лог)')
plt.title('Лайки vs Просмотры')
plt.grid(True, which="both", ls="--", linewidth=0.5)

views_95 = df['views'].quantile(0.95)
df_filtered = df[df['views'] <= views_95]

plt.subplot(1, 2, 2)
sns.boxplot(data=df_filtered, x='category_id', y='views')
plt.yscale('log')
plt.xlabel('ID категории')
plt.ylabel('Просмотры (лог)')
plt.title('Просмотры по категориям\n(без верхних 5% выбросов)')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 6))
corr_cols = ['views', 'likes', 'dislikes', 'comment_count']
corr = df[corr_cols].corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', center=0, square=True)
plt.title('Корреляция между метриками видео')
plt.show()

print("\nТОП-5 категорий по количеству видео:")
print(df['category_id'].value_counts().head())

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

df['log_views'] = np.log1p(df['views'])

feature_cols = ['likes', 'dislikes', 'comment_count', 'category_id']

X = df[feature_cols].copy()
y = df['log_views'].copy()

X = X.dropna()
y = y.loc[X.index]

print(f" Форма признаков X: {X.shape}")
print(f" Форма целевой переменной y: {y.shape}")

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("\n Данные готовы к обучению!")
print(f"Train size: {X_train_scaled.shape[0]}")
print(f"Test size:  {X_test_scaled.shape[0]}")

plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.hist(df['views'], bins=50, color='skyblue')
plt.title('Исходное распределение views')
plt.xlabel('Просмотры')

plt.subplot(1, 2, 2)
plt.hist(df['log_views'], bins=50, color='lightgreen')
plt.title('Распределение log(1 + views)')
plt.xlabel('log(1 + просмотры)')

plt.tight_layout()
plt.show()

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt

model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    layers.Dropout(0.3),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(32, activation='relu'),
    layers.Dense(1)
])

model.compile(
    optimizer='adam',
    loss='mse',
    metrics=['mae']
)

history = model.fit(
    X_train_scaled, y_train,
    epochs=20,
    batch_size=256,
    validation_split=0.2,
    verbose=1
)

y_pred = model.predict(X_test_scaled).flatten()

y_test_original = np.expm1(y_test)
y_pred_original = np.expm1(y_pred)

r2 = r2_score(y_test_original, y_pred_original)
print(f"\n R² на тестовом наборе (в исходных просмотрах): {r2:.4f}")

if r2 >= 0.6:
    print("Цель достигнута: R² ≥ 0.6!")
else:
    print("Цель не достигнута. Нужно улучшать модель.")

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss (MSE) при обучении')
plt.xlabel('Эпоха')
plt.ylabel('MSE')
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.scatter(y_test_original, y_pred_original, alpha=0.3, s=10)
plt.plot([y_test_original.min(), y_test_original.max()],
         [y_test_original.min(), y_test_original.max()], 'r--', lw=2)
plt.xlabel('Истинные просмотры')
plt.ylabel('Предсказанные просмотры')
plt.title(f'Предсказание vs Истина\nR² = {r2:.3f}')
plt.xscale('log')
plt.yscale('log')
plt.grid(True)

plt.tight_layout()
plt.show()

from sklearn.ensemble import RandomForestRegressor

n_samples = min(10000, len(X))
idx = X.sample(n=n_samples, random_state=42).index

X_sample = X.loc[idx]
y_sample = y.loc[idx]

rf_model = RandomForestRegressor(
    n_estimators=50,
    max_depth=10,
    random_state=42,
    n_jobs=-1
)
rf_model.fit(X_sample, y_sample)

importances = pd.Series(rf_model.feature_importances_, index=feature_cols)
importances_sorted = importances.sort_values(ascending=False)

print("Факторы, влияющие на количество просмотров (по log(1+views)):")
for i, (feature, imp) in enumerate(importances_sorted.items(), 1):
    print(f"{i}. {feature}: {imp:.3f}")

importances_sorted.plot(kind='barh', figsize=(8, 4), color='teal')
plt.title('Важность признаков (ускоренная модель — Random Forest)')
plt.xlabel('Относительная важность')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()